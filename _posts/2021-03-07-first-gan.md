---
layout: post
title: My first GAN!
tags: [GAN]
excerpt_separator: <!--more-->
---

I've been hearing about GANs and seeing lots of incredibly well generated images from GANs in the past years, but I've never got around training one. Recently I finally have something that I want to generate, and something that GANs should be able to generate - textures.

There're a few papers on texture synthesis out there, but this one called [Spatial GAN](https://arxiv.org/pdf/1611.08207.pdf) caught my eye - mostly because of the simplicity of it. It's an earlier GAN paper and the architecture and training still mostly follows DCGAN. However, the architecture is fully convolutional, and can be used to generate different sizes of texture images at test time.

So, let's give it a try!

Let's start with this single texture from the [DTD dataset](https://www.robots.ox.ac.uk/~vgg/data/dtd/), and use random crops from it to train the GAN:

<p align="center"><img src="/assets/img/first_gan/pink_texture.png"/></p>

The first architecture I have is quite shallow: the crops from the real texture is 64x64, and the input noise to the generator has spatial dimension 16x16. The generator only scales up the noise 4x (per side), from 16x16 to 64x64.

These are some textures generated by this model, with size 64x64:

<p align="center"><img src="/assets/img/first_gan/generated1.png"/></p>

Just for fun, I mess with the standard deviation when sampling the noise for the generator. We can see the texture goes from smooth to having more constrast, with increasing stddev:

<p align="center"><img src="/assets/img/first_gan/generated2.png"/></p>

And, most crucial of all, how about the promise that we could generate texture of different sizes? Let's make something large.

<p align="center"><img src="/assets/img/first_gan/generated3.png"/></p>

Well, that's not bad, but due to how shallow the networks are, the structure of the original texture (fabric-like grids) is not captured. Let's try again with deeper networks. This time, the noise has spatial dimensions 4x4, and scale up 16x. The result:

<p align="center"><img src="/assets/img/first_gan/generated4.png"/></p>

Looks like a nice piece of fabric :)

With more textures in the training set, we can also generate a range of textures. To map different textures into different latent space, instead of directly sampling the noise from a normal distribution, I separate the sampling process into two steps. First, a "texture type" is sampled from a normal distribution. This noise vector is repeated spatially (since the same texture is repeated through out the image). Then, we use the "texture type" as mean to sample from another Gaussian, with smaller standard deviation. This time, all spatial vectors are sampled independently.

And there we go:

<p align="center"><img src="/assets/img/first_gan/generated5.png"/></p>

That was my first GAN experience. A few thoughts:
- Following the recommendations in the [DCGAN paper](https://arxiv.org/pdf/1511.06434.pdf), it was pretty straightforward to get something working (as long as you don't forget to normalize the value range of the real images like me ;P)
- There were a few trials where the discriminator was overpowering and winning all the time (loss of the discriminator stays at ~0). Keeping the generator and the discriminator "mirrored" of each other works pretty well.
- Generating larger textures (than the size used in training) doesn't always work well. It depends on the texture's global structure and also the network architecture. I'm still looking into this, hoping I'll have more to say soon.
